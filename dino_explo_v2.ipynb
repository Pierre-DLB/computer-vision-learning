{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchinfo import summary\n",
    "from PIL import ImageOps\n",
    "import random\n",
    "import torch.functional as F\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# device = \"cuda\"\n",
    "device= \"mps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solarization(object):\n",
    "    \"\"\"\n",
    "    Apply Solarization to the PIL image.\n",
    "    \"\"\"\n",
    "    def __init__(self, p):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if random.random() < self.p:\n",
    "            img = transforms.ToPILImage(img)\n",
    "            return transforms.ToTensor(ImageOps.solarize(img))\n",
    "        else:\n",
    "            return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset : Fashion MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torchvision.datasets.FashionMNIST(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "test_data = torchvision.datasets.FashionMNIST(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_dl = DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "\n",
    "dummy = next(iter(train_dl))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 3x3 images in train set\n",
    "\n",
    "def plot(images, labels, names):\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(7, 7))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(images[i], cmap='gray')\n",
    "        ax.axis('off')\n",
    "        ax.set_title(names[labels[i].item()], fontsize=14)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(images=train_data.data[:9], labels=train_data.targets[:9], names=train_data.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforming = transforms.RandomResizedCrop(size=(28, 28), scale=(0.5, 0.99), ratio=(3/4, 4/3), interpolation=2)\n",
    "x1= dummy[3]\n",
    "print(\"transforming=\", transforming)\n",
    "print(\"x1.shape_ \", x1.shape)\n",
    "\n",
    "plt.imshow(transforming(x1).squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Construction du modèle\n",
    "\n",
    "Le modèle est un ViT. Réimplémentons donc les ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSA(nn.Module):\n",
    "    \"\"\" Implement multi-headed self-attention.\"\"\"\n",
    "    def __init__(self, dim_embedding, n_heads, dh=None) -> None:\n",
    "        super().__init__()\n",
    "        self.dim_embedding = dim_embedding\n",
    "        self.n_heads = n_heads\n",
    "        self.dh = dh or dim_embedding // n_heads\n",
    "        self.qkv = nn.Linear(dim_embedding, 3 * (n_heads * self.dh))\n",
    "        self.merge = nn.Linear(n_heads * self.dh, dim_embedding)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, _ = x.shape\n",
    "        qkv = self.qkv(x)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        \n",
    "        q = q.view(batch_size, seq_length, self.n_heads, self.dh).transpose(1, 2)\n",
    "        k = k.view(batch_size, seq_length, self.n_heads, self.dh).transpose(1, 2)\n",
    "        v = v.view(batch_size, seq_length, self.n_heads, self.dh).transpose(1, 2)\n",
    "        \n",
    "        A = torch.softmax(torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.dh, dtype=torch.float32)), dim=-1)\n",
    "        out = torch.matmul(A, v)\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_length, -1)\n",
    "        return self.merge(out)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim_embedding) -> None:\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim_embedding, 4*dim_embedding)\n",
    "        self.fc2 = nn.Linear(4*dim_embedding, dim_embedding)\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim_embedding, n_heads, dh=None):\n",
    "        super().__init__()\n",
    "        self.MSAH = MSA(dim_embedding, n_heads, dh)\n",
    "        self.MLP = MLP(dim_embedding)\n",
    "        self.layer_norm1 = nn.LayerNorm(dim_embedding)\n",
    "        self.layer_norm2 = nn.LayerNorm(dim_embedding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm1(x)\n",
    "        x = x + self.MSAH(x)\n",
    "        x = self.layer_norm2(x)\n",
    "        x = self.MLP(x) + x\n",
    "        return x\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_embedding, patch_size, num_patches=None, channels = 1):\n",
    "        super().__init__()\n",
    "        num_patches = num_patches or (28 // patch_size)**2\n",
    "        self.conv = nn.Conv2d(channels, dim_embedding, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim_embedding))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches+1, dim_embedding))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = torch.flatten(self.conv(x), start_dim = 2).permute(0, 2, 1)\n",
    "        \n",
    "        cls = self.cls_token.repeat(B, 1, 1)\n",
    "        pos_embedding = self.pos_embedding.repeat(B, 1, 1)\n",
    "\n",
    "        x = torch.cat([cls, x], dim=1) + pos_embedding\n",
    "        return x\n",
    "    \n",
    "    # def interpolate_pos_encoding(self, x, w, h):\n",
    "    #     npatch = x.shape[1] - 1\n",
    "    #     N = self.pos_embed.shape[1] - 1\n",
    "    #     if npatch == N and w == h:\n",
    "    #         return self.pos_embed\n",
    "    #     class_pos_embed = self.pos_embed[:, 0]\n",
    "    #     patch_pos_embed = self.pos_embed[:, 1:]\n",
    "    #     dim = x.shape[-1]\n",
    "    #     w0 = w // self.patch_embed.patch_size\n",
    "    #     h0 = h // self.patch_embed.patch_size\n",
    "    #     # we add a small number to avoid floating point error in the interpolation\n",
    "    #     # see discussion at https://github.com/facebookresearch/dino/issues/8\n",
    "    #     w0, h0 = w0 + 0.1, h0 + 0.1\n",
    "    #     patch_pos_embed = nn.functional.interpolate(\n",
    "    #         patch_pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2),\n",
    "    #         scale_factor=(w0 / math.sqrt(N), h0 / math.sqrt(N)),\n",
    "    #         mode='bicubic',\n",
    "    #     )\n",
    "    #     assert int(w0) == patch_pos_embed.shape[-2] and int(h0) == patch_pos_embed.shape[-1]\n",
    "    #     patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
    "    #     return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)\n",
    "\n",
    "    \n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, dim_embedding, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.MLP = nn.Sequential(nn.LayerNorm(dim_embedding),\n",
    "                                 nn.Linear(dim_embedding, hidden_dim), \n",
    "                                 nn.GELU(), \n",
    "                                #  nn.Linear(hidden_dim, hidden_dim),\n",
    "                                #  nn.GELU()\n",
    "                                 )\n",
    "        self.last_layer= nn.utils.weight_norm(nn.Linear(hidden_dim, num_classes, bias=False))\n",
    "        self.last_layer.weight_g.data.fill_(1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.MLP(x)\n",
    "        return self.last_layer(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, dim_embedding, n_heads, patch_size, n_blocks, n_hidden, n_out, dh=None, num_patches=None, channels=1):\n",
    "        super().__init__()\n",
    "        self.embedding = PatchEmbed(dim_embedding, patch_size, num_patches, channels)\n",
    "        self.blocks = nn.Sequential(*nn.ModuleList([Block(dim_embedding, n_heads, dh) for _ in range(n_blocks)]))\n",
    "        self.projection_head = ProjectionHead(dim_embedding, n_hidden, n_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        X = self.embedding(x)\n",
    "        X = self.blocks(X)\n",
    "        return self.projection_head(X[:, 0])\n",
    "        \n",
    "    def embed(self, x):\n",
    "        return self.embedding(x)\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        return self.blocks(self.embedding(x))\n",
    "    \n",
    "    @property\n",
    "    def extract_DINO(self):\n",
    "        return nn.Sequential(self.embedding, self.blocks)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "vit = VisionTransformer(dim_embedding=64, n_heads=8, patch_size=4, n_blocks=6, n_hidden=512, n_out=100)\n",
    "dummy_embed = vit.embed(dummy)\n",
    "print(vit(dummy))\n",
    "\n",
    "summary(vit, input_size=(64, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Build crops (large and small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methodology : \n",
    "\n",
    "Build large and small crops of the image with data augmentations.\n",
    "- 2 large crops : $x_g1$ and $x_g2$\n",
    "- k small crops : __TO DO__ : Update the positionnal and patch embedding, do split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import Any\n",
    "\n",
    "\n",
    "# class DataAugmentation:\n",
    "#     def __init__(self, global_crops_scale, local_crops_scale, local_crops_number, large_crop_size=28) -> None:\n",
    "#         col_jit = transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)\n",
    "        \n",
    "#         flip_and_color_jitter= transforms.Compose(\n",
    "#             transforms.RandomHorizontalFlip(p=0.5),\n",
    "#             transforms.RandomApply(col_jit ,p=0.8),\n",
    "#             transforms.RandomGrayscale(p=0.2))\n",
    "        \n",
    "#         normalize = transforms.Compose(\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.Normalize(0.456, 0.225)\n",
    "#         )\n",
    "\n",
    "#         self.global_transform1 = transforms.Compose(\n",
    "#                         transforms.RandomResizedCrop(large_crop_size, scale=global_crops_scale, interpolation=2),\n",
    "#             flip_and_color_jitter,\n",
    "#             # no gaussian blur, bad enough quality...\n",
    "#             normalize\n",
    "#         )\n",
    "        \n",
    "#         self.global_transform2 = transforms.Compose(\n",
    "#             transforms.RandomResizedCrop(large_crop_size, scale=global_crops_scale, interpolation=2),\n",
    "#             flip_and_color_jitter,\n",
    "#             # no gaussian blur, bad enough quality...\n",
    "#             Solarization(p=0.2),\n",
    "#             normalize\n",
    "#         )\n",
    "\n",
    "#         self.local_crops_number = local_crops_number\n",
    "#         self.local_transfo = transforms.Compose([\n",
    "#             transforms.RandomResizedCrop(4, scale=local_crops_scale, interpolation=Image.BICUBIC),\n",
    "#             flip_and_color_jitter,\n",
    "#             normalize,\n",
    "#         ])\n",
    "\n",
    "#     def __call__(self, *args: Any, **kwds: Any) ->:\n",
    "\n",
    "class DataAugLarge:\n",
    "    def __init__(self, global_crops_scale, large_crop_size=28) -> None:\n",
    "        \n",
    "        flip_and_color_jitter= transforms.Compose(\n",
    "            [\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomApply(\n",
    "                    [transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)]\n",
    "                        ,p=0.8),\n",
    "            ])\n",
    "        \n",
    "        # normalize = transforms.Compose(\n",
    "        #     [\n",
    "        #         transforms.ToTensor(),\n",
    "        #         # ToDtype(dtype=torch.float32),\n",
    "        #         transforms.Normalize(0.456, 0.225)\n",
    "        #     ]\n",
    "        # )\n",
    "        normalize = transforms.Normalize(0.456, 0.225)\n",
    "\n",
    "        self.global_transform1 = transforms.Compose(\n",
    "                        [transforms.RandomResizedCrop(large_crop_size, scale=global_crops_scale, interpolation=2),\n",
    "            flip_and_color_jitter,\n",
    "            # no gaussian blur, bad enough quality...\n",
    "            normalize\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.global_transform2 = transforms.Compose(\n",
    "            [transforms.RandomResizedCrop(large_crop_size, scale=global_crops_scale, interpolation=2),\n",
    "            flip_and_color_jitter,\n",
    "            # no gaussian blur, bad enough quality...\n",
    "            Solarization(p=0.2),\n",
    "            normalize]\n",
    "        )\n",
    "\n",
    "        # self.local_crops_number = local_crops_number\n",
    "        # self.local_transfo = transforms.Compose([\n",
    "        #     transforms.RandomResizedCrop(4, scale=local_crops_scale, interpolation=Image.BICUBIC),\n",
    "        #     flip_and_color_jitter,\n",
    "        #     normalize,\n",
    "        # ])\n",
    "\n",
    "\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = x #images are in Black&White, add a dim for the channels\n",
    "        return self.global_transform1(x), self.global_transform2(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = DataAugLarge(large_crop_size=(28, 28), global_crops_scale=(0.5, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1, d2 = trans(dummy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Construction de la training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit(d1), vit(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DinoLoss(nn.Module):\n",
    "    \"\"\" Custom Dino Loss - Hard Labels. (cf DeiT paper)\"\"\"\n",
    "    def __init__(self, tpt, tps, C, m=0.9):\n",
    "        super().__init__()\n",
    "        self.tps= tps\n",
    "        self.tpt = tpt\n",
    "        self.C = C\n",
    "        self.m = m\n",
    "        pass\n",
    "    def H(self, t, s):\n",
    "        t = t.detach() # stop the gradient computation\n",
    "        s = nn.functional.softmax(s/self.tps, dim=1) # sharpen softmax\n",
    "        t = nn.functional.softmax((t-self.C)/self.tpt, dim=1) # Center + sharpen softmax\n",
    "        return -(t * torch.log(s)).sum(dim=1).mean()\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def update_center(self, teacher_out, m=None):\n",
    "        if m is None:\n",
    "            m=self.m\n",
    "        self.C = m * self.C + (1-m) * teacher_out.mean(dim=0)\n",
    "    \n",
    "    def forward(self, t1, t2, s1, s2, m=None):\n",
    "        loss= self.H(t1, s2)/2 + self.H(t2, s1)/2\n",
    "        self.update_center(torch.cat([t1, t2]), m) # cat on dim=0\n",
    "        return loss\n",
    "    \n",
    "    @property\n",
    "    def reset_C(self):\n",
    "        self.C = torch.zeros(self.C.shape, device=self.C.device)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = DinoLoss(tpt=0.04, tps=0.1, C= torch.zeros(100), m=0.9 )\n",
    "li = loss_fn(vit(d1), vit(d2), vit(d2), vit(d1))\n",
    "\n",
    "loss_fn.reset_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(teacher, student, optimizer, dataloader, augment, loss_fn, device, l=0.996, params=None):\n",
    "    \"\"\"\n",
    "    Train one epoch\n",
    "\n",
    "    Parameters :\n",
    "    teacher : nn.Module\n",
    "        The teacher model\n",
    "    student : nn.Module\n",
    "        The student model\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        The optimizer\n",
    "    dataloader : torch.utils.data.DataLoader\n",
    "        The train dataloader\n",
    "    augment : callable\n",
    "        The data augmentation function\n",
    "    loss_fn : callable\n",
    "        The loss function\n",
    "    device : str\n",
    "        The device to use for training\n",
    "    l : float\n",
    "        The momentum for the teacher update\n",
    "    params : dict\n",
    "        The parameters for the training (fine tune, change in parameters, etc.)\n",
    "    \"\"\"\n",
    "    teacher.train()\n",
    "    teacher = teacher.to(device)\n",
    "    student.train()\n",
    "    student = student.to(device)\n",
    "\n",
    "    loss_list = []\n",
    "    ### TO DO ### : Add Metrics tracking\n",
    "\n",
    "    for x, _ in dataloader:\n",
    "        x1, x2 = augment(x), augment(x)\n",
    "        s1, s2 = student(x1.to(device)), student(x2.to(device))\n",
    "        t1, t2 = teacher(x1.to(device)), teacher(x2.to(device))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(t1, t2, s1, s2) # if m in params.keys : add m in the execution\n",
    "        # C is updated in the forward pass of the loss\n",
    "        loss.backward()\n",
    "        student.step()\n",
    " \n",
    "        # Teacher update : \n",
    "        # The update rule is a cosine sechedule with l going from 0.996 to 1.0\n",
    "        with torch.no_grad():\n",
    "            # l = \n",
    "            for param_q, param_k in zip(student.parameters(), teacher.parameters()):\n",
    "                param_k.data.mul_(l).add_((1 - l) * param_q.detach().data)\n",
    "        \n",
    "        loss_list.append(loss.mean(dim=0).item())\n",
    "        \n",
    "    return loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = DinoLoss(tpt=0.04, tps=0.1, C= torch.zeros(100), m=0.9 )\n",
    "\n",
    "augment = DataAugLarge(large_crop_size=(28, 28), global_crops_scale=(0.5, 1.0))\n",
    "dataloader = train_dl\n",
    "\n",
    "Teacher = VisionTransformer(dim_embedding=64, n_heads=8, patch_size=4, n_blocks=6, n_hidden=512, n_out=100)\n",
    "Student = VisionTransformer(dim_embedding=64, n_heads=8, patch_size=4, n_blocks=6, n_hidden=512, n_out=100)\n",
    "\n",
    "optimizer = torch.optim.AdamW(Student.parameters(), lr=0.001)\n",
    "# device = \"cuda\"\n",
    "device = \"mps\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one_epoch(teacher=Teacher, \n",
    "                student=Student, \n",
    "                optimizer=optimizer, \n",
    "                dataloader=dataloader, \n",
    "                augment=augment, \n",
    "                loss_fn=loss_fn, \n",
    "                device=device, \n",
    "                l=0.996, \n",
    "                params=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, _ = next(iter(dataloader))\n",
    "x1, x2 = augment(x), augment(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment(dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "headmind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
