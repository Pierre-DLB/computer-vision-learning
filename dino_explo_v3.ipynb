{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchinfo import summary\n",
    "from PIL import ImageOps\n",
    "import random\n",
    "import torch.functional as F\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "device = \"cuda\"\n",
    "# device= \"mps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solarization(object):\n",
    "    \"\"\"\n",
    "    Apply Solarization to the PIL image.\n",
    "    \"\"\"\n",
    "    def __init__(self, p):\n",
    "        self.p = p\n",
    "\n",
    "    # def __call__(self, img):\n",
    "    #     if random.random() < self.p:\n",
    "    #         img = transforms.ToPILImage()(img)\n",
    "    #         return transforms.ToTensor()(ImageOps.solarize(img))\n",
    "    #     else:\n",
    "    #         return img\n",
    "        \n",
    "    def __call__(self, img_batch):\n",
    "        if img_batch.dim()> 3:\n",
    "            for img in img_batch:\n",
    "                if random.random() < self.p:\n",
    "                    img = transforms.ToPILImage()(img)\n",
    "                    img = transforms.ToTensor()(ImageOps.solarize(img))\n",
    "            return img_batch\n",
    "        else:\n",
    "            if random.random() < self.p:\n",
    "                img = transforms.ToPILImage()(img)\n",
    "                img = transforms.ToTensor()(ImageOps.solarize(img))\n",
    "            return img_batch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset : Fashion MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torchvision.datasets.FashionMNIST(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "test_data = torchvision.datasets.FashionMNIST(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "train_dl = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_dl = DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "def load_data(batch_size):\n",
    "    train_data = torchvision.datasets.FashionMNIST(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    "    )\n",
    "\n",
    "    test_data = torchvision.datasets.FashionMNIST(\n",
    "        root='./data',\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transforms.ToTensor()\n",
    "    )\n",
    "\n",
    "    train_dl = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    test_dl = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "    return train_dl, test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = next(iter(train_dl))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 3x3 images in train set\n",
    "\n",
    "def plot(images, labels, names):\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(7, 7))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(images[i], cmap='gray')\n",
    "        ax.axis('off')\n",
    "        ax.set_title(names[labels[i].item()], fontsize=14)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(images=train_data.data[:9], labels=train_data.targets[:9], names=train_data.classes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Construction du modèle\n",
    "\n",
    "Le modèle est un ViT. Réimplémentons donc les ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSA(nn.Module):\n",
    "    \"\"\" Implement multi-headed self-attention.\"\"\"\n",
    "    def __init__(self, dim_embedding, n_heads, dh=None) -> None:\n",
    "        super().__init__()\n",
    "        self.dim_embedding = dim_embedding\n",
    "        self.n_heads = n_heads\n",
    "        self.dh = dh or dim_embedding // n_heads\n",
    "        self.qkv = nn.Linear(dim_embedding, 3 * (n_heads * self.dh))\n",
    "        self.merge = nn.Linear(n_heads * self.dh, dim_embedding)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, _ = x.shape\n",
    "        qkv = self.qkv(x)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        \n",
    "        q = q.view(batch_size, seq_length, self.n_heads, self.dh).transpose(1, 2)\n",
    "        k = k.view(batch_size, seq_length, self.n_heads, self.dh).transpose(1, 2)\n",
    "        v = v.view(batch_size, seq_length, self.n_heads, self.dh).transpose(1, 2)\n",
    "        \n",
    "        A = torch.softmax(torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.dh, dtype=torch.float32)), dim=-1)\n",
    "        out = torch.matmul(A, v)\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_length, -1)\n",
    "        return self.merge(out)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim_embedding) -> None:\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim_embedding, 4*dim_embedding)\n",
    "        self.fc2 = nn.Linear(4*dim_embedding, dim_embedding)\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim_embedding, n_heads, dh=None):\n",
    "        super().__init__()\n",
    "        self.MSAH = MSA(dim_embedding, n_heads, dh)\n",
    "        self.MLP = MLP(dim_embedding)\n",
    "        self.layer_norm1 = nn.LayerNorm(dim_embedding)\n",
    "        self.layer_norm2 = nn.LayerNorm(dim_embedding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm1(x)\n",
    "        x = x + self.MSAH(x)\n",
    "        x = self.layer_norm2(x)\n",
    "        x = self.MLP(x) + x\n",
    "        return x\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_embedding, patch_size, num_patches=None, channels = 1):\n",
    "        super().__init__()\n",
    "        num_patches = num_patches or (28 // patch_size)**2\n",
    "        self.conv = nn.Conv2d(channels, dim_embedding, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim_embedding))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches+1, dim_embedding))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = torch.flatten(self.conv(x), start_dim = 2).permute(0, 2, 1)\n",
    "        \n",
    "        cls = self.cls_token.repeat(B, 1, 1)\n",
    "        pos_embedding = self.pos_embedding.repeat(B, 1, 1)\n",
    "\n",
    "        x = torch.cat([cls, x], dim=1) + pos_embedding\n",
    "        return x\n",
    "    \n",
    "    # def interpolate_pos_encoding(self, x, w, h):\n",
    "    #     npatch = x.shape[1] - 1\n",
    "    #     N = self.pos_embed.shape[1] - 1\n",
    "    #     if npatch == N and w == h:\n",
    "    #         return self.pos_embed\n",
    "    #     class_pos_embed = self.pos_embed[:, 0]\n",
    "    #     patch_pos_embed = self.pos_embed[:, 1:]\n",
    "    #     dim = x.shape[-1]\n",
    "    #     w0 = w // self.patch_embed.patch_size\n",
    "    #     h0 = h // self.patch_embed.patch_size\n",
    "    #     # we add a small number to avoid floating point error in the interpolation\n",
    "    #     # see discussion at https://github.com/facebookresearch/dino/issues/8\n",
    "    #     w0, h0 = w0 + 0.1, h0 + 0.1\n",
    "    #     patch_pos_embed = nn.functional.interpolate(\n",
    "    #         patch_pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2),\n",
    "    #         scale_factor=(w0 / math.sqrt(N), h0 / math.sqrt(N)),\n",
    "    #         mode='bicubic',\n",
    "    #     )\n",
    "    #     assert int(w0) == patch_pos_embed.shape[-2] and int(h0) == patch_pos_embed.shape[-1]\n",
    "    #     patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
    "    #     return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)\n",
    "\n",
    "    \n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, dim_embedding, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.MLP = nn.Sequential(nn.LayerNorm(dim_embedding),\n",
    "                                 nn.Linear(dim_embedding, hidden_dim), \n",
    "                                 nn.GELU(), \n",
    "                                #  nn.Linear(hidden_dim, hidden_dim),\n",
    "                                #  nn.GELU()\n",
    "                                 )\n",
    "        self.last_layer= nn.utils.weight_norm(nn.Linear(hidden_dim, num_classes, bias=False))\n",
    "        self.last_layer.weight_g.data.fill_(1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.MLP(x)\n",
    "        return self.last_layer(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, dim_embedding, n_heads, patch_size, n_blocks, n_hidden, n_out, dh=None, num_patches=None, channels=1):\n",
    "        super().__init__()\n",
    "        self.embedding = PatchEmbed(dim_embedding, patch_size, num_patches, channels)\n",
    "        self.blocks = nn.Sequential(*nn.ModuleList([Block(dim_embedding, n_heads, dh) for _ in range(n_blocks)]))\n",
    "        self.projection_head = ProjectionHead(dim_embedding, n_hidden, n_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        X = self.embedding(x)\n",
    "        X = self.blocks(X)\n",
    "        return self.projection_head(X[:, 0])\n",
    "        \n",
    "    def embed(self, x):\n",
    "        return self.embedding(x)\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        return self.blocks(self.embedding(x))\n",
    "    \n",
    "    @property\n",
    "    def extract_DINO(self):\n",
    "        return nn.Sequential(self.embedding, self.blocks)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "vit = VisionTransformer(dim_embedding=64, n_heads=8, patch_size=4, n_blocks=6, n_hidden=512, n_out=100)\n",
    "dummy_embed = vit.embed(dummy)\n",
    "print(vit(dummy))\n",
    "summary(vit, input_size=(64, 1, 28, 28))\n",
    "del vit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Build crops (large and small)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methodology : \n",
    "\n",
    "Build large and small crops of the image with data augmentations.\n",
    "- 2 large crops : $x_g1$ and $x_g2$\n",
    "- k small crops : __TO DO__ : Update the positionnal and patch embedding, do split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import Any\n",
    "\n",
    "\n",
    "# class DataAugmentation:\n",
    "#     def __init__(self, global_crops_scale, local_crops_scale, local_crops_number, large_crop_size=28) -> None:\n",
    "#         col_jit = transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)\n",
    "        \n",
    "#         flip_and_color_jitter= transforms.Compose(\n",
    "#             transforms.RandomHorizontalFlip(p=0.5),\n",
    "#             transforms.RandomApply(col_jit ,p=0.8),\n",
    "#             transforms.RandomGrayscale(p=0.2))\n",
    "        \n",
    "#         normalize = transforms.Compose(\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.Normalize(0.456, 0.225)\n",
    "#         )\n",
    "\n",
    "#         self.global_transform1 = transforms.Compose(\n",
    "#                         transforms.RandomResizedCrop(large_crop_size, scale=global_crops_scale, interpolation=2),\n",
    "#             flip_and_color_jitter,\n",
    "#             # no gaussian blur, bad enough quality...\n",
    "#             normalize\n",
    "#         )\n",
    "        \n",
    "#         self.global_transform2 = transforms.Compose(\n",
    "#             transforms.RandomResizedCrop(large_crop_size, scale=global_crops_scale, interpolation=2),\n",
    "#             flip_and_color_jitter,\n",
    "#             # no gaussian blur, bad enough quality...\n",
    "#             Solarization(p=0.2),\n",
    "#             normalize\n",
    "#         )\n",
    "\n",
    "#         self.local_crops_number = local_crops_number\n",
    "#         self.local_transfo = transforms.Compose([\n",
    "#             transforms.RandomResizedCrop(4, scale=local_crops_scale, interpolation=Image.BICUBIC),\n",
    "#             flip_and_color_jitter,\n",
    "#             normalize,\n",
    "#         ])\n",
    "\n",
    "#     def __call__(self, *args: Any, **kwds: Any) ->:\n",
    "\n",
    "class DataAugLarge:\n",
    "    def __init__(self, global_crops_scale, large_crop_size=28) -> None:\n",
    "        \n",
    "        flip_and_color_jitter= transforms.Compose(\n",
    "            [\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomApply(\n",
    "                    [transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)]\n",
    "                        ,p=0.8),\n",
    "            ])\n",
    "        \n",
    "        # normalize = transforms.Compose(\n",
    "        #     [\n",
    "        #         transforms.ToTensor(),\n",
    "        #         # ToDtype(dtype=torch.float32),\n",
    "        #         transforms.Normalize(0.456, 0.225)\n",
    "        #     ]\n",
    "        # )\n",
    "        normalize = transforms.Normalize(0.456, 0.225)\n",
    "\n",
    "        self.global_transform1 = transforms.Compose(\n",
    "                        [transforms.RandomResizedCrop(large_crop_size, scale=global_crops_scale, interpolation=2),\n",
    "            flip_and_color_jitter,\n",
    "            # no gaussian blur, bad enough quality...\n",
    "            normalize\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.global_transform2 = transforms.Compose(\n",
    "            [transforms.RandomResizedCrop(large_crop_size, scale=global_crops_scale, interpolation=2),\n",
    "            flip_and_color_jitter,\n",
    "            # no gaussian blur, bad enough quality...\n",
    "            Solarization(p=0.2),\n",
    "            normalize]\n",
    "        )\n",
    "\n",
    "        # self.local_crops_number = local_crops_number\n",
    "        # self.local_transfo = transforms.Compose([\n",
    "        #     transforms.RandomResizedCrop(4, scale=local_crops_scale, interpolation=Image.BICUBIC),\n",
    "        #     flip_and_color_jitter,\n",
    "        #     normalize,\n",
    "        # ])\n",
    "\n",
    "\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = x #images are in Black&White, add a dim for the channels\n",
    "        return self.global_transform1(x), self.global_transform2(x)\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing if everything works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = DataAugLarge(large_crop_size=(28, 28), global_crops_scale=(0.5, 1.0))\n",
    "d1, d2 = trans(dummy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Construction de la training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DinoLoss(nn.Module):\n",
    "    \"\"\" Custom Dino Loss - Hard Labels. (cf DeiT paper)\"\"\"\n",
    "    def __init__(self, tpt, tps, C, m=0.9):\n",
    "        super().__init__()\n",
    "        self.tps= tps\n",
    "        self.tpt = tpt\n",
    "        self.C = C\n",
    "        self.m = m\n",
    "        pass\n",
    "    def H(self, t, s):\n",
    "        t = t.detach() # stop the gradient computation\n",
    "        s = nn.functional.softmax(s/self.tps, dim=1) # sharpen softmax\n",
    "        t = nn.functional.softmax((t-self.C)/self.tpt, dim=1) # Center + sharpen softmax\n",
    "        return -(t * torch.log(s)).sum(dim=1).mean()\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def update_center(self, teacher_out, m=None):\n",
    "        if m is None:\n",
    "            m=self.m\n",
    "        self.C = m * self.C + (1-m) * teacher_out.mean(dim=0)\n",
    "    \n",
    "    def forward(self, t1, t2, s1, s2, m=None):\n",
    "        loss= self.H(t1, s2)/2 + self.H(t2, s1)/2\n",
    "        self.update_center(torch.cat([t1, t2]), m) # cat on dim=0\n",
    "        return loss\n",
    "    \n",
    "    @property\n",
    "    def reset_C(self):\n",
    "        self.C = torch.zeros(self.C.shape, device=self.C.device)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing if everything works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit = VisionTransformer(dim_embedding=64, n_heads=8, patch_size=4, n_blocks=6, n_hidden=512, n_out=100)\n",
    "loss_fn = DinoLoss(tpt=0.04, tps=0.1, C= torch.zeros(100), m=0.9 )\n",
    "li = loss_fn(vit(d1), vit(d2), vit(d2), vit(d1))\n",
    "\n",
    "loss_fn.reset_C\n",
    "del vit\n",
    "del loss_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(teacher, student, optimizer, dataloader, augment, loss_fn, device, l=0.996, params=None, verbose=True):\n",
    "    \"\"\"\n",
    "    Train one epoch\n",
    "\n",
    "    Parameters :\n",
    "    teacher : nn.Module\n",
    "        The teacher model\n",
    "    student : nn.Module\n",
    "        The student model\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        The optimizer\n",
    "    dataloader : torch.utils.data.DataLoader\n",
    "        The train dataloader\n",
    "    augment : callable\n",
    "        The data augmentation function\n",
    "    loss_fn : callable\n",
    "        The loss function\n",
    "    device : str\n",
    "        The device to use for training\n",
    "    l : float\n",
    "        The momentum for the teacher update\n",
    "    params : dict\n",
    "        The parameters for the training (fine tune, change in parameters, etc.)\n",
    "    \"\"\"\n",
    "    teacher.train()\n",
    "    teacher = teacher.to(device)\n",
    "    student.train()\n",
    "    student = student.to(device)\n",
    "    loss_fn.C = loss_fn.C.to(device)\n",
    "\n",
    "    loss_list = []\n",
    "    ### TO DO ### : Add Metrics tracking\n",
    "\n",
    "    for i, (x, _) in enumerate(dataloader):\n",
    "        x1, x2 = augment(x)\n",
    "        x1, x2 = x1.to(device), x2.to(device)\n",
    "\n",
    "        s1, s2 = student(x1), student(x2)\n",
    "        t1, t2 = teacher(x1), teacher(x2)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(t1, t2, s1, s2) # if m in params.keys : add m in the execution\n",
    "        # C is updated in the forward pass of the loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    " \n",
    "        # Teacher update : \n",
    "        # The update rule is a cosine sechedule with l going from 0.996 to 1.0\n",
    "        with torch.no_grad():\n",
    "            # l = \n",
    "            for param_q, param_k in zip(student.parameters(), teacher.parameters()):\n",
    "                param_k.data.mul_(l).add_((1 - l) * param_q.detach().data)\n",
    "        \n",
    "        loss_list.append(loss.mean(dim=0).item())\n",
    "        if verbose:\n",
    "            print(f\"Batch {i+1} done\")\n",
    "\n",
    "        \n",
    "    return loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 3072\n",
    "\n",
    "loss_fn = DinoLoss(tpt=0.04, tps=0.1, C= torch.zeros(100), m=0.9 )\n",
    "augment = DataAugLarge(large_crop_size=(28, 28), global_crops_scale=(0.5, 1.0))\n",
    "Teacher = VisionTransformer(dim_embedding=64, n_heads=8, patch_size=4, n_blocks=6, n_hidden=512, n_out=100)\n",
    "Student = VisionTransformer(dim_embedding=64, n_heads=8, patch_size=4, n_blocks=6, n_hidden=512, n_out=100)\n",
    "\n",
    "\n",
    "def dataloader_(batchsize):\n",
    "    return DataLoader(train_data, batch_size=batchsize, shuffle=True)\n",
    "\n",
    "def lr_(batchsize):\n",
    "    # Paper : lr = 0.0005*batchsize/256\n",
    "    return (0.0005*batchsize/256)\n",
    "\n",
    "\n",
    "dataloader = dataloader_(BATCH_SIZE)\n",
    "optimizer = torch.optim.AdamW(Student.parameters(), lr=lr_(BATCH_SIZE))\n",
    "\n",
    "\n",
    "device = \"cuda\"\n",
    "# device = \"mps\"\n",
    "\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# train_one_epoch(teacher=Teacher, \n",
    "#                 student=Student, \n",
    "#                 optimizer=optimizer, \n",
    "#                 dataloader=dataloader, \n",
    "#                 augment=augment, \n",
    "#                 loss_fn=loss_fn, \n",
    "#                 device=device, \n",
    "#                 l=0.996, \n",
    "#                 params=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_wrapper(epochs, teacher, student, optimizer, dataloader, augment, loss_fn, device, l=0.996, params=None, fun_epoch=None):\n",
    "    \n",
    "    agg_loss = []\n",
    "    agg_std = []\n",
    "    for epoch in range(epochs):\n",
    "        loss = train_one_epoch(teacher, student, optimizer, dataloader, augment, loss_fn, device, l, params, verbose=False)\n",
    "        ep_loss = np.mean(loss)\n",
    "        batch_std = np.std(loss)\n",
    "        agg_loss.append(ep_loss)\n",
    "        agg_std.append(batch_std)\n",
    "        print(f\"Epoch {epoch+1} done, Loss : {ep_loss} +/- {batch_std}\")\n",
    "\n",
    "        if fun_epoch is not None: # function to do at the end of each epoch, to collect metrics for instance.\n",
    "            fun_epoch(teacher, student, dataloader, device)\n",
    "\n",
    "    return agg_loss, agg_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std = training_wrapper(epochs=4,\n",
    "                teacher=Teacher, \n",
    "                student=Student, \n",
    "                optimizer=optimizer, \n",
    "                dataloader=dataloader, \n",
    "                augment=augment, \n",
    "                loss_fn=loss_fn, \n",
    "                device=device, \n",
    "                l=0.996, \n",
    "                params=None,\n",
    "                fun_epoch=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have these lists\n",
    "epochs = list(range(1, len(mean) + 1))  # Creating a list of epochs\n",
    "# mean = [...]  # Your list of mean values\n",
    "# std = [...]   # Your list of standard deviation values\n",
    "\n",
    "# Set the style for the plot\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(x=epochs, y=mean, color='red', linewidth=2)\n",
    "plt.fill_between(epochs, np.array(mean) - np.array(std), np.array(mean) + np.array(std), color='grey', alpha=0.3)\n",
    "\n",
    "# Set the title and labels\n",
    "plt.title(\"Evolution of DINO average Loss per epochs with batch losses standard deviation on Fashion MNIST\", fontsize=14)\n",
    "plt.xlabel(\"Epochs\", fontsize=12)\n",
    "plt.ylabel(\"Loss\", fontsize=12)\n",
    "\n",
    "# Adjust the layout and display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Test the performance of the model on a knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function to apply the model to teh data to get embeddingns and then use the embedgings to train a knn classifier\n",
    "\n",
    "def get_embeddings(model, dataloader, device):\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for i, (x, y) in enumerate(dataloader):\n",
    "            x = x.to(device)\n",
    "            embeddings.append(model(x).cpu())\n",
    "            labels.append(y.cpu())\n",
    "    embeddings = torch.cat(embeddings)\n",
    "    labels = torch.cat(labels)\n",
    "    return embeddings, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_embeddingsS, train_labelsS = get_embeddings(model=Student, dataloader=train_dl, device=device)\n",
    "test_embeddingsS, test_labelsS = get_embeddings(model=Student, dataloader=test_dl, device=device)\n",
    "print(\"train:\", train_embeddingsS.shape, train_labelsS.shape)\n",
    "print(\"test:\", test_embeddingsS.shape, test_labelsS.shape)\n",
    "\n",
    "\n",
    "train_embeddingsT, train_labelsT = get_embeddings(model=Teacher, dataloader=train_dl, device=device)\n",
    "test_embeddingsT, test_labelsT = get_embeddings(model=Teacher, dataloader=test_dl, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def test_knn(k, train_embeddings=train_embeddingsT, train_labels= train_labelsT, test_embeddings=test_embeddingsT, test_labels=test_labelsT):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(train_embeddings, train_labels)\n",
    "    print(\"train set :\", knn.score(train_embeddings, train_labels))\n",
    "    print(\"test set :\", knn.score(test_embeddings, test_labels))\n",
    "    return knn.score(train_embeddings, train_labels), knn.score(test_embeddings, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_perfs = []\n",
    "test_perfs = []\n",
    "it = []\n",
    "\n",
    "for k in range(10):\n",
    "    print(f\"K={k+1}\")\n",
    "    t1, t2 = test_knn(k+1)\n",
    "    train_perfs.append(t1)\n",
    "    test_perfs.append(t2)\n",
    "    it.append(k)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_knn_performance(train_perfs, test_perfs):\n",
    "    # Create a range of k values (assuming they start from 1)\n",
    "    k_values = range(1, len(train_perfs) + 1)\n",
    "    \n",
    "    # Set the style for the plot\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot train performances\n",
    "    sns.lineplot(x=k_values, y=train_perfs, color='red', label='Train')\n",
    "    \n",
    "    # Plot test performances\n",
    "    sns.lineplot(x=k_values, y=test_perfs, color='blue', label='Test')\n",
    "    \n",
    "    # Set the title\n",
    "    plt.title(\"Performance of a kNN trained with the DINO embeddings on the train set,\\n\"\n",
    "              \"and evaluated on the test set. Embeddings are given by the teacher\", \n",
    "              fontsize=14)\n",
    "    \n",
    "    # Set the axis labels\n",
    "    plt.xlabel(\"Number of neighbours taken in kNN\", fontsize=12)\n",
    "    plt.ylabel(\"Score (%)\", fontsize=12)\n",
    "    \n",
    "    # Add a legend\n",
    "    plt.legend(fontsize=10)\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# train_perfs = [0.85, 0.87, 0.89, 0.90, 0.91]\n",
    "# test_perfs = [0.82, 0.84, 0.85, 0.84, 0.83]\n",
    "# plot_knn_performance(train_perfs, test_perfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_knn_performance(train_perfs, test_perfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openTSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from openTSNE import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have your data in a numpy array called 'data'\n",
    "# and labels in a numpy array called 'labels'\n",
    "\n",
    "# Initialize and fit the t-SNE model\n",
    "tsne = TSNE(\n",
    "    n_components=2,\n",
    "    perplexity=30,\n",
    "    metric=\"euclidean\",\n",
    "    n_jobs=8,  # Adjust this based on your CPU cores\n",
    "    random_state=42,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Fit the model and transform the data\n",
    "embedding = tsne.fit(train_embeddingsT)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(embedding[:, 0], embedding[:, 1], c=labelsT, cmap='viridis', alpha=0.7)\n",
    "plt.colorbar(scatter)\n",
    "plt.title(\"t-SNE visualization of high-dimensional data\")\n",
    "plt.xlabel(\"t-SNE 1\")\n",
    "plt.ylabel(\"t-SNE 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(embedding[:, 0], embedding[:, 1], c=train_labelsT, cmap='viridis', alpha=0.7)\n",
    "plt.colorbar(scatter)\n",
    "plt.title(\"t-SNE visualization of the traing dataset embedding. Color is the label\")\n",
    "plt.xlabel(\"t-SNE 1\")\n",
    "plt.ylabel(\"t-SNE 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a confusion matrix with colors mapping\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=8)\n",
    "knn.fit(train_embeddingsT, train_labelsT)\n",
    "\n",
    "y_pred = knn.predict(test_embeddingsT)\n",
    "y_pred = y_pred.astype(int)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(confusion_matrix(test_labelsT, y_pred), annot=True, cmap=\"Blues\")\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion matrix of the KNN classifier (K=8) trained on dino embeddings')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# train_data.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(Teacher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn.predict(test_embeddingsT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddingsS.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddingsT.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 3072\n",
    "\n",
    "loss_fn = DinoLoss(tpt=0.04, tps=0.1, C= torch.zeros(100), m=0.9 )\n",
    "augment = DataAugLarge(large_crop_size=(28, 28), global_crops_scale=(0.5, 1.0))\n",
    "Teacher = VisionTransformer(dim_embedding=64, n_heads=8, patch_size=4, n_blocks=6, n_hidden=512, n_out=100)\n",
    "Student = VisionTransformer(dim_embedding=64, n_heads=8, patch_size=4, n_blocks=6, n_hidden=512, n_out=100)\n",
    "\n",
    "\n",
    "def dataloader_(batchsize):\n",
    "    return DataLoader(train_data, batch_size=batchsize, shuffle=True)\n",
    "\n",
    "def lr_(batchsize):\n",
    "    # Paper : lr = 0.0005*batchsize/256\n",
    "    return (0.0005*batchsize/256)\n",
    "\n",
    "\n",
    "dataloader = dataloader_(BATCH_SIZE)\n",
    "optimizer = torch.optim.AdamW(Student.parameters(), lr=lr_(BATCH_SIZE))\n",
    "\n",
    "\n",
    "device = \"cuda\"\n",
    "# device = \"mps\"\n",
    "\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Understand the underlying mechanics : "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try play with the rates:\n",
    "- output dim ? (set @100 for now)\n",
    "- learning rate (set @0.01 for now)\n",
    "- update parameters for centering (m) and exponential moving average (lambda :l) for the loss\n",
    "- number of layers in the model\n",
    "- sharpening temperatures ? (set tpt=0.04, tps=0.1, for now)\n",
    "\n",
    "batch size is not the most concern for now.\n",
    "Implementing the small crops ?\n",
    "\n",
    "\n",
    "\n",
    "Question I would like to know : what is the \"average shift\" per epoch in the model ? (take a point, and get embedding after each epoch, compute the difference (shift) and then see the average fro several points or others ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "headmind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
