{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torchinfo import summary\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# device = \"cuda\"\n",
    "device= \"mps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gdown\n",
    "# !pip install scipy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We are going to use the Calthech101 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvertToRGB(object):\n",
    "    def __call__(self, image):\n",
    "        if image.mode == 'RGB':\n",
    "            return image\n",
    "        elif image.mode == 'L':\n",
    "            return image.convert('RGB')\n",
    "        else:\n",
    "            raise ValueError(f'Unsupported image mode: {image.mode}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "transform = T.Compose([\n",
    "    ConvertToRGB(),\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    # Add other transforms as needed\n",
    "])\n",
    "\n",
    "dataset = torchvision.datasets.Caltech101(root=Path.cwd(), \n",
    "                                          target_type = 'category', \n",
    "                                          transform = transform, \n",
    "                                          target_transform = None, \n",
    "                                          download= True)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = np.random.randint(0, len(dataset), (3,3))\n",
    "\n",
    "fig, axes = plt.subplots(3,3)\n",
    "\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        img, _ = dataset[indexes[i,j]]\n",
    "        axes[i,j].imshow(img.permute(1,2,0))\n",
    "\n",
    "# Dataset presentation : \n",
    "print(f\"the dataset includes {len(dataset)} samples\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 768\n",
    "N = 100\n",
    "\n",
    "z = torch.randn((N, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.shape\n",
    "# z.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, D_input, D_h):\n",
    "        super().__init__()\n",
    "        self.D_h = D_h\n",
    "        self.D_input=D_input\n",
    "\n",
    "        self.q_mat = nn.Linear(in_features=self.D_input, out_features=self.D_h, bias=None)\n",
    "        self.k_mat = nn.Linear(in_features=self.D_input, out_features=self.D_h, bias=None)\n",
    "        self.v_mat = nn.Linear(in_features=self.D_input, out_features=self.D_h, bias=None)\n",
    "    \n",
    "    def forward(self, z):\n",
    "        q, k, v = self.q_mat(z), self.k_mat(z), self.v_mat(z)\n",
    "        A = torch.softmax(torch.matmul(q, torch.transpose(k, 1, 0)) / torch.sqrt(torch.tensor(self.D_h)), axis=1)\n",
    "        return torch.matmul(A, v)\n",
    "        \n",
    "\n",
    "# class MSA(nn.Module): # PROBLEM WITH MEMORY MANAGEMENT BECAUSE OF THE LIST\n",
    "#     def __init__(self, embedding_dim, num_heads):\n",
    "#         super().__init__()\n",
    "#         self.k = num_heads\n",
    "#         self.D_input = embedding_dim\n",
    "#         self.D_h = embedding_dim//num_heads\n",
    "\n",
    "#         self.attentions = [SelfAttention(self.D_h, self.D_h) for i in range(self.k)]\n",
    "#         self.unification_matrix = nn.Linear(self.D_input, self.D_input, bias=None)\n",
    "    \n",
    "#     def forward(self, z):\n",
    "#         vectors = torch.split(z, split_size_or_sections=self.D_h, dim=1)\n",
    "\n",
    "#         for i in range(self.k):\n",
    "#             vectors[i] = self.attentions[i](vectors[i])\n",
    "\n",
    "#         MSA = torch.cat(vectors, dim=1)\n",
    "#         return self.unification_matrix(MSA)\n",
    "    \n",
    "\n",
    "class MSA(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embedding_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.query = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.key = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.value = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.fc_out = nn.Linear(embedding_dim, embedding_dim)\n",
    "        # for q, k, v, instead of having k linear layers acting in parallel with shape nn.Linear(embedding_dim, embedding_dim/k), \n",
    "        # they are all stacked together in one layer, and we cut the output afterwards.\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, seq_length, embedding_dim = x.shape\n",
    "\n",
    "        # Split the embedding into self.num_heads different pieces\n",
    "        # view : see the vector reshaped as \"\", without copying i.e. do specific operations easily\n",
    "        queries = self.query(x).view(N, seq_length, self.num_heads, self.head_dim)\n",
    "        keys = self.key(x).view(N, seq_length, self.num_heads, self.head_dim)\n",
    "        values = self.value(x).view(N, seq_length, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose to get dimensions (N, num_heads, seq_length, head_dim)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Calculate the attention scores\n",
    "        scores = torch.matmul(queries, keys.transpose(-2, -1)) * self.scale\n",
    "        attention = torch.softmax(scores, dim=-1)\n",
    "\n",
    "        # Get the weighted values\n",
    "        out = torch.matmul(attention, values)\n",
    "\n",
    "        # Reshape to (N, seq_length, embedding_dim)\n",
    "        out = out.transpose(1, 2).contiguous().view(N, seq_length, embedding_dim)\n",
    "\n",
    "        # Apply the final linear layer (unification layer)\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, embedding_dim, mlp_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.mlp_size = mlp_size\n",
    "        \n",
    "        self.block = nn.Sequential(nn.LayerNorm(normalized_shape=self.embedding_dim),\n",
    "                        nn.Linear(in_features=embedding_dim, out_features=mlp_size),\n",
    "                        nn.GELU(),\n",
    "                        nn.Dropout(dropout),\n",
    "                        nn.Linear(in_features=mlp_size, out_features=embedding_dim),\n",
    "                        nn.Dropout(dropout))\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.block(z)\n",
    "\n",
    "\n",
    "# Main difference between Self attention and MSA : get less computations in matrix multiplications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa = SelfAttention(D, D)\n",
    "print(sa(z).shape)\n",
    "print(f\"Q: {sa.q_mat._parameters['weight'].shape}, K: {sa.k_mat._parameters['weight'].shape}, V: {sa.q_mat._parameters['weight'].shape}\")\n",
    "\n",
    "summary(sa)\n",
    "\n",
    "# msa = MSA(embedding_dim=768, num_heads=16)\n",
    "# output_attention = msa.forward(z)\n",
    "# print(output_attention.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msa = MSA(embedding_dim=768, num_heads=16)\n",
    "\n",
    "summary(msa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                embedding_dim:int=768,\n",
    "                num_heads:int=12,\n",
    "                mlp_size:int=3072, \n",
    "                mlp_dropout:float=0.1, \n",
    "                attn_dropout:float=0):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.mlp_size = mlp_size\n",
    "        self.mlp_dropout = mlp_dropout\n",
    "        self.attn_dropout = attn_dropout\n",
    "\n",
    "        self.norm = nn.LayerNorm(normalized_shape=self.embedding_dim)\n",
    "\n",
    "        self.MSA = MSA(self.embedding_dim, self.num_heads)\n",
    "        self.MLP = MLP(embedding_dim=self.embedding_dim, mlp_size=self.mlp_size, dropout=self.mlp_dropout)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        att_x = self.MSA(self.norm(x)) + x\n",
    "\n",
    "        mlp_x = self.MLP(self.norm(att_x)) + x \n",
    "        return mlp_x\n",
    "\n",
    "class PatchEmbedder(nn.Module):\n",
    "\n",
    "    def __init__(self, patch_size: int, embedding_dim:int = 768, patch_num=196, random=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.patch_num = patch_num\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=3, out_channels=embedding_dim, \n",
    "                              kernel_size=(patch_size, patch_size), stride=patch_size, padding=0)\n",
    "                            # donne en gros le meme resultat que couper en petits patches, flatten et passer dans une linear layer\n",
    "        self.flat = nn.Flatten(start_dim=2, end_dim=3)\n",
    "\n",
    "        # ADD [CLASS] TOKEN\n",
    "        if random:\n",
    "            self.class_embedding= nn.Parameter(torch.rand((1, 1, embedding_dim)), requires_grad=True)\n",
    "        else:\n",
    "            self.class_embedding= nn.Parameter(torch.ones((1, 1, embedding_dim)), requires_grad=True)\n",
    "\n",
    "        # ADD POSITION EMBEDDING\n",
    "        if random:\n",
    "            self.pos_embedding= nn.Parameter(torch.ones((1, patch_num+1 , embedding_dim)), requires_grad=True)\n",
    "        else:\n",
    "            self.pos_embedding= nn.Parameter(torch.rand((1, patch_num+1 , embedding_dim)), requires_grad=True)\n",
    "        # Add the position embedding to the patch and class token embedding\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "\n",
    "        y = self.flat(self.conv(x))\n",
    "        y = y.permute(0, 2, 1)\n",
    "        \n",
    "        return torch.cat((self.class_embedding.expand(batch_size, -1, -1), y),dim=1) + self.pos_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(next(iter(dataloader))[0][0].permute(1, 2, 0))\n",
    "\n",
    "embedder = PatchEmbedder(patch_size=16, embedding_dim=768)\n",
    "embedder(next(iter(dataloader))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, test_loader, criterion, optimizer, n_epochs):\n",
    "    # Dictionary to store losses\n",
    "    losses = {'train_loss': [], 'test_loss': []}\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        running_train_loss = 0.0\n",
    "        \n",
    "        # Training loop\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()  # Zero the parameter gradients\n",
    "            \n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Compute loss\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Optimize the model parameters\n",
    "            \n",
    "            running_train_loss += loss.item() * inputs.size(0)  # Accumulate loss\n",
    "        \n",
    "        epoch_train_loss = running_train_loss / len(train_loader.dataset)\n",
    "        losses['train_loss'].append(epoch_train_loss)\n",
    "        \n",
    "        # Validation loop\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        running_test_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():  # Disable gradient computation\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                outputs = model(inputs)  # Forward pass\n",
    "                loss = criterion(outputs, labels)  # Compute loss\n",
    "                \n",
    "                running_test_loss += loss.item() * inputs.size(0)  # Accumulate loss\n",
    "        \n",
    "        epoch_test_loss = running_test_loss / len(test_loader.dataset)\n",
    "        losses['test_loss'].append(epoch_test_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}, Train Loss: {epoch_train_loss:.4f}, Test Loss: {epoch_test_loss:.4f}\")\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# Example usage:\n",
    "# model = MyModel().to(device)\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "# n_epochs = 10\n",
    "# losses = train(model, train_loader, test_loader, criterion, optimizer, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self,\n",
    "                    img_size:int=224, # Training resolution from Table 3 in ViT paper\n",
    "                    in_channels:int=3, # Number of channels in input image\n",
    "                    patch_size:int=16, # Patch size\n",
    "                    num_transformer_layers:int=12, # Layers from Table 1 for ViT-Base\n",
    "                    embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base\n",
    "                    mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n",
    "                    num_heads:int=12, # Heads from Table 1 for ViT-Base\n",
    "                    attn_dropout:float=0, # Dropout for attention projection\n",
    "                    mlp_dropout:float=0.1, # Dropout for dense/MLP layers\n",
    "                    embedding_dropout:float=0.1, # Dropout for patch and position embeddings\n",
    "                    num_classes:int=1000): # Default for ImageNet but can customize this\n",
    "        super().__init__() # don't forget the super().__init__()!\n",
    "        assert img_size % patch_size == 0, f\"Image size must be divisible by patch size, image size: {img_size}, patch size: {patch_size}.\"\n",
    "\n",
    "        self.img_size = img_size\n",
    "        self.in_channels = in_channels\n",
    "        self.patch_size = patch_size\n",
    "        self.patch_num = (self.img_size//self.patch_size)**2\n",
    "\n",
    "        self.num_transformer_layers = num_transformer_layers\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.mlp_size = mlp_size\n",
    "        self.num_heads = num_heads\n",
    "        self.attn_dropout = attn_dropout\n",
    "        self.mlp_dropout = mlp_dropout\n",
    "        self.embedding_dropout = embedding_dropout\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.Embedding = PatchEmbedder(patch_size=self.patch_size,\n",
    "                                        embedding_dim=self.embedding_dim,\n",
    "                                        patch_num=self.patch_num, random=True)\n",
    "        \n",
    "        self.EmbeddingDropout = nn.Dropout(p=self.embedding_dropout)\n",
    "        \n",
    "        self.StackedEncoders = nn.Sequential(*[Encoder(embedding_dim=self.embedding_dim, \n",
    "                                                num_heads=self.num_heads,\n",
    "                                                mlp_size=self.mlp_size,\n",
    "                                                mlp_dropout=self.mlp_dropout,\n",
    "                                                attn_dropout=self.attn_dropout)\n",
    "                                        for layer in range(self.num_transformer_layers)])\n",
    "        self.Classifier = nn.Sequential(\n",
    "        nn.LayerNorm(normalized_shape=embedding_dim),\n",
    "        nn.Linear(in_features=embedding_dim, out_features=num_classes)\n",
    "        )\n",
    "\n",
    "        def forward(x):\n",
    "            \n",
    "            embedded_x = self.EmbeddingDropout(self.Embedding(x))\n",
    "\n",
    "            embedded_x = self.StackedEncoders(embedded_x)\n",
    "            \n",
    "            class_x = self.Classifier(x)\n",
    "\n",
    "            return class_x[:,0]\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit= ViT(num_classes=len(dataset.categories))\n",
    "summary(model=vit)\n",
    "#         input_size=(32, 3, 224, 224), # (batch_size, color_channels, height, width)\n",
    "#         # col_names=[\"input_size\"], # uncomment for smaller output\n",
    "#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#         col_width=20,\n",
    "#         row_settings=[\"var_names\"]\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Natixis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
